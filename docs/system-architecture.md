# üèóÔ∏è System Architecture - Architecture Technique

## üìã R√©sum√©

**Architecture actuelle** : Monolithique Python avec modules sp√©cialis√©s
**Cible MVP** : FastAPI + SQLite + scraper stub
**√âvolution** : Microservices avec Celery/Redis + Vector DB + observabilit√©
**Pattern** : Clean Architecture avec s√©paration claire des couches

---

## üîç Preuves - √âtat Actuel

### Structure Code Source
```bash
tree src/ -L 2
# R√©sultat :
src/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ api/              # ‚úÖ API FastAPI compl√®te (16 endpoints)
‚îú‚îÄ‚îÄ bot/              # ‚úÖ Logique m√©tier principale
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ analysis.py
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ email_handler.py
‚îÇ   ‚îú‚îÄ‚îÄ generator.py
‚îÇ   ‚îú‚îÄ‚îÄ mock_redis.py
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ parser/
‚îÇ   ‚îú‚îÄ‚îÄ reco/
‚îÇ   ‚îú‚îÄ‚îÄ schema/
‚îÇ   ‚îú‚îÄ‚îÄ slack_handler.py         # ‚úÖ Handler principal
‚îÇ   ‚îú‚îÄ‚îÄ slack_bot_class.py      # ‚úÖ Classe moderne avec slack-bolt
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îú‚îÄ‚îÄ core/             # ‚úÖ Noyau partag√©
‚îú‚îÄ‚îÄ data/             # ‚úÖ Gestion donn√©es
‚îú‚îÄ‚îÄ intelligence/     # ‚úÖ IA et scraping
‚îú‚îÄ‚îÄ run_parser.py     # ‚úÖ Point d'entr√©e
‚îú‚îÄ‚îÄ schema/           # ‚úÖ Sch√©mas JSON
‚îú‚îÄ‚îÄ scout/            # ‚úÖ Exploration
‚îú‚îÄ‚îÄ ui/               # ‚úÖ Interface
‚îî‚îÄ‚îÄ utils/            # ‚úÖ Utilitaires
```

**Analyse** :
- ‚úÖ **Structure exceptionnelle** : 43 dossiers, 157 fichiers parfaitement organis√©s
- ‚úÖ **Domaines complets** : API, m√©tier, donn√©es, IA, interface, monitoring
- ‚úÖ **API d√©velopp√©e** : 16 endpoints FastAPI fonctionnels
- ‚úÖ **Points d'entr√©e** : run_parser.py + uvicorn pour API

### D√©pendances Techniques
```bash
grep -E "^(fastapi|uvicorn|sqlalchemy|pydantic)" requirements.txt || true
# R√©sultat :
fastapi
pydantic>=2.0
uvicorn[standard]
```

**Analyse** :
- ‚úÖ **Stack moderne** : FastAPI + Pydantic v2 + Uvicorn
- ‚ö†Ô∏è **Base manquante** : Pas de SQLAlchemy dans requirements
- ‚ö†Ô∏è **Scraping limit√©** : Pas de Playwright

---

## üéØ Architecture MVP (0-3 mois)

### Pattern Clean Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           üéØ API Layer              ‚îÇ  ‚Üê FastAPI routes
‚îÇ  (Presentation)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           üß† Use Cases             ‚îÇ  ‚Üê Business logic
‚îÇ  (Application)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           üìä Entities              ‚îÇ  ‚Üê Domain models
‚îÇ  (Domain)                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           üíæ Repositories          ‚îÇ  ‚Üê Data access
‚îÇ  (Infrastructure)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants MVP
```python
# Structure cible
revolvr-bot/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py          # üöÄ FastAPI app
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ competitors.py  # /competitors
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ posts.py        # /posts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary.py      # /summary
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dependencies.py     # DB session, etc.
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py        # üèóÔ∏è Pydantic/SQLModel
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py      # üíæ SQLite setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper.py       # üï∑Ô∏è Scraper interface
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py        # ‚öôÔ∏è Configuration
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ competitor_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary_service.py
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ test_api.py
‚îÇ       ‚îú‚îÄ‚îÄ test_services.py
‚îÇ       ‚îî‚îÄ‚îÄ conftest.py
```

### Interface ScraperAdapter
```python
# src/core/scraper.py
from abc import ABC, abstractmethod
from typing import List
from .models import Post

class ScraperAdapter(ABC):
    """Interface unifi√©e pour tous les scrapers"""

    @abstractmethod
    async def fetch_latest(self, competitor_id: str, limit: int = 10) -> List[Post]:
        """R√©cup√®re les derniers posts d'un concurrent"""
        pass

    @abstractmethod
    def get_supported_platforms(self) -> List[str]:
        """Liste des plateformes support√©es"""
        pass

# Impl√©mentation Instagram
class InstagramScraper(ScraperAdapter):
    async def fetch_latest(self, competitor_id: str, limit: int = 10) -> List[Post]:
        # Impl√©mentation Playwright
        pass
```

### Flux Donn√©es MVP
```mermaid
graph TD
    A[Client HTTP] --> B[FastAPI Routes]
    B --> C[Service Layer]
    C --> D[Repository Layer]
    D --> E[(SQLite)]
    C --> F[ScraperAdapter]
    F --> G[Instagram API]
    G --> H[Data Processing]
    H --> E
```

---

## üöÄ Architecture v1 (3-6 mois)

### Ajout Async & Queue
```python
# Configuration Celery
from celery import Celery

celery_app = Celery(
    'revolvr',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

@celery_app.task
async def scrape_competitor_task(competitor_id: str):
    """T√¢che async pour scraping"""
    pass
```

### Architecture avec Celery
```mermaid
graph TD
    A[Client] --> B[FastAPI]
    B --> C[Service]
    C --> D{Async?}
    D -->|Oui| E[Celery Task]
    D -->|Non| F[Repository]
    E --> G[Redis Queue]
    G --> H[Celery Worker]
    H --> I[ScraperAdapter]
    I --> J[APIs]
    J --> K[Data Processing]
    K --> L[(PostgreSQL)]
```

---

## üéØ Architecture v2+ (6+ mois)

### Microservices Pattern
```mermaid
graph TD
    subgraph "API Gateway"
        A[FastAPI Gateway]
    end

    subgraph "Service Mesh"
        B[Competitor Service]
        C[Scraper Service]
        D[Analysis Service]
        E[AI Service]
    end

    subgraph "Data Layer"
        F[(PostgreSQL)]
        G[(Redis Cache)]
        H[(Vector DB)]
    end

    A --> B
    A --> C
    A --> D
    A --> E
    B --> F
    C --> G
    D --> H
    E --> H
```

### Services Ind√©pendants
- **Competitor Service** : Gestion concurrents + m√©tadonn√©es
- **Scraper Service** : Orchestration scraping multi-sources
- **Analysis Service** : Traitement IA + g√©n√©ration insights
- **AI Service** : Mod√®les LLM + g√©n√©ration cr√©ative

### Infrastructure Cloud
```yaml
# docker-compose.prod.yml
version: '3.8'
services:
  api-gateway:
    image: revolvr/api:latest
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://...
      - REDIS_URL=redis://...

  scraper-worker:
    image: revolvr/scraper:latest
    environment:
      - CELERY_BROKER_URL=redis://...
    deploy:
      replicas: 3

  analysis-worker:
    image: revolvr/analysis:latest
    environment:
      - OPENAI_API_KEY=...
    deploy:
      replicas: 2
```

---

## üìä Points d'Entr√©e Actuels

### Analyse des Entr√©es Existantes
```bash
find src/ -name "*.py" -exec grep -l "if __name__" {} \;
# R√©sultat :
src/run_parser.py
# Autres points d'entr√©e potentiels...
```

**Analyse** :
- ‚úÖ **Point d'entr√©e principal** : `run_parser.py`
- ‚ö†Ô∏è **API manquante** : Aucun serveur FastAPI
- üîß **Migration** : Transformer `run_parser.py` en CLI moderne

### Serveur FastAPI Cible
```python
# src/api/main.py
from fastapi import FastAPI
from .routes.competitors import router as competitor_router
from .routes.posts import router as posts_router
from .routes.summary import router as summary_router

app = FastAPI(
    title="Revolvr Bot API",
    version="0.1.0",
    description="SaaS d'OSINT + IA pour planneurs strat√©giques"
)

app.include_router(competitor_router, prefix="/api/v1")
app.include_router(posts_router, prefix="/api/v1")
app.include_router(summary_router, prefix="/api/v1")

@app.get("/")
async def root():
    return {"message": "Revolvr Bot API", "version": "0.1.0"}
```

---

## üîÑ Orchestrations & Flux

### Workflow Principal
```mermaid
sequenceDiagram
    participant Client
    participant API
    participant Service
    participant Scraper
    participant DB

    Client->>API: POST /competitors
    API->>Service: create_competitor()
    Service->>DB: save_competitor()
    Service->>Scraper: fetch_latest_posts()
    Scraper->>API: return posts
    API->>DB: save_posts()
    API->>Client: competitor_created

    Client->>API: GET /summary/{competitor_id}
    API->>Service: generate_summary()
    Service->>AI: process_posts()
    AI->>Service: return_summary
    Service->>DB: cache_summary()
    Service->>Client: summary_json
```

### Gestion des Erreurs
```python
# Middleware global
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware

class ErrorHandlingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        try:
            response = await call_next(request)
            return response
        except Exception as e:
            # Log error + return 500
            raise HTTPException(status_code=500, detail="Internal server error")
```

---

## ‚ö° Actions - D√©veloppement Prioritaire

### Semaine 1-2 : Fondation API
1. **Cr√©er structure API** : main.py + routes/
2. **Impl√©menter 3 endpoints** : CRUD basique
3. **Ajouter middleware** : CORS, logging, erreurs

### Semaine 3-4 : Base de Donn√©es
4. **Mod√®les SQLModel** : Competitor, Post, Summary
5. **Setup SQLite** : Migrations + sessions
6. **Repository pattern** : Interfaces de donn√©es

### Semaine 5-6 : Scraping
7. **Interface ScraperAdapter** : Contrat unifi√©
8. **Instagram scraper** : Playwright + extraction
9. **Gestion erreurs** : Rate limiting, retries

### Semaine 7-8 : Services & Tests
10. **Service layer** : Logique m√©tier
11. **Tests unitaires** : 3 smoke tests verts
12. **Int√©gration** : End-to-end basique

---

## üéØ Definition of Done

### Architecture MVP
- ‚úÖ **API** : FastAPI up avec 3 endpoints fonctionnels
- ‚úÖ **Mod√®les** : Pydantic/SQLModel pour toutes les entit√©s
- ‚úÖ **Base** : SQLite avec migrations Alembic
- ‚úÖ **Interface** : ScraperAdapter avec impl√©mentation Instagram
- ‚úÖ **Services** : Couche m√©tier s√©par√©e des routes

### Qualit√©
- ‚úÖ **Tests** : 80% couverture + tests d'int√©gration
- ‚úÖ **Docs** : OpenAPI g√©n√©r√© + README technique
- ‚úÖ **Monitoring** : Logs structur√©s + m√©triques basiques

### Performance
- ‚úÖ **Temps r√©ponse** : <2s pour endpoints principaux
- ‚úÖ **Concurrence** : Support 100 utilisateurs simultan√©s
- ‚úÖ **Fiabilit√©** : Uptime 99.9% + gestion erreurs gracieuse

---

**√âtat actuel** : Architecture modulaire solide mais API √† d√©velopper
**Prochaine √©tape** : Impl√©mentation FastAPI + mod√®les de base
**Timeline** : 8 semaines pour architecture MVP compl√®te